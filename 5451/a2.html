<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2023-04-27 Thu 17:06 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>CSCI 5451 Assignment 2: Distributed and Shared Memory Programming</title>
<meta name="author" content="Chris Kauffman" />
<meta name="generator" content="Org Mode" />
<style>
  #content { max-width: 60em; margin: auto; }
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #e6e6e6;
    border-radius: 3px;
    background-color: #f2f2f2;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: auto;
  }
  pre.src:before {
    display: none;
    position: absolute;
    top: -8px;
    right: 12px;
    padding: 3px;
    color: #555;
    background-color: #f2f2f299;
  }
  pre.src:hover:before { display: inline; margin-top: 14px;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-authinfo::before { content: 'Authinfo'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-hledger:before { content: 'hledger'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .equation-container {
    display: table;
    text-align: center;
    width: 100%;
  }
  .equation {
    vertical-align: middle;
  }
  .equation-label {
    display: table-cell;
    text-align: right;
    vertical-align: middle;
  }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { }
</style>
<meta name="viewport" content="width=device-width, maximum-scale=1, minimum-scale=1" />
<style type="text/css">
@media screen {
:root {
--heading-bg-color:#e8c62e;
--heading-fg-color:#7a0019;
}
html {
font-family: serif;
text-align: justify;
}
pre.src, pre.example {
overflow-x: scroll;
}
/* Merge subtitle area with title area */
.subtitle {
text-align: center;
margin-top: -2em;
padding-top: 1em;
padding-bottom: 0.1em;
}
.title, .subtitle {
color: var(--heading-fg-color);
background-color: var(--heading-bg-color);
}
/* Section borders, left section header style */
div.outline-2, #table-of-contents {
background-color: rgb(250,250,250);
border: 0.75em solid var(--heading-bg-color);
border-top: 0em;
padding: 0em .5em .5em .5em; /* top right bottom left */
margin: 1em 0em 1em 0em; /* top right bottom left */
}
div.outline-2 > h2, #table-of-contents > h2 {
background-color: var(--heading-bg-color);
color: var(--heading-fg-color);
font-variant: small-caps;
padding: 0em 0em 0em .5em; /* top right bottom left */
margin: 0em -.5em 0em -.75em; /* top right bottom left */
text-align: left;
}
blockquote {
font-style: italic;
}
td, th {
padding-top: 2px;
padding-bottom: 2px;
}
body {
background-color: #EEE;
}
pre {
}
#content, #preamble, #postamble {
margin-left:300px;
max-width: 100%;
}
.tag {
background-color: inherit; font-family: inherit;
padding: inherit; font-size: 80%; font-weight: inherit;
text-transform: uppercase;
}

.figure p { text-align: inherit; }
figure-number { font-style: italic; }
#table-of-contents {
text-align: left;
position: fixed;
left: 0;
margin: 0 auto;
padding: 0;
width: 300px;
top: 0;
height: 100%;
border: 0;
display: block;
}
#text-table-of-contents {
overflow-y: scroll;
height: 100%;
}
#text-table-of-contents ul {
padding-left: 1em;
margin-left: 0.5em;
}
#table-of-contents > h2 {
padding: 0.1em;
margin: 0;
}
/* adjustments for small screen, toc at top only */
@media (max-width: 800px) { /* landscape for iphone */
html {
-webkit-text-size-adjust: none;  /* prevent scaling of text on mobile */
}
body {
background-color: #EEE;
width:100%;
margin:0 auto;
}
#content, #preamble, #postamble {
margin-left:0;
}
#table-of-contents {
position: static;
left: inherit;
width:inherit;
height: auto;
border-top: 0em;
padding: 0em .5em .5em .5em; /* top right bottom left */
margin: 1em 0em 1em 0em; /* top right bottom left */
border: 0.75em solid #006633;
}
div.outline-2, #table-of-contents {
background-color: rgb(250,250,250);
border: 0.75em solid var(--heading-bg-color);
border-top: 0em;
padding: 0em .5em .5em .5em; /* top right bottom left */
margin: 1em 0em 1em 0em; /* top right bottom left */
}
div.outline-2 > h2, #table-of-contents > h2 {
background-color: var(--heading-bg-color);
color: var(--heading-fg-color);
font-variant: small-caps;
padding: 0em 0em 0em .5em; /* top right bottom left */
margin: 0em -.5em 0em -.75em; /* top right bottom left */
text-align: left;
}
#text-table-of-contents {
overflow-y: visible;
height: inherit;
}
#text-table-of-contents ul {
padding-left: 1em;
margin-left: 0.5em;
}
}
.linenr { font-size: xx-small; }
}

@media print {
html {
font-family: serif;
font-size: 10pt;
text-align: justify;
.linenr { font-size: xx-small; }
}
}
</style>
</head>
<body>
<div id="preamble" class="status">
<i>Last Updated: 2023-04-27 Thu 17:06</i>
</div>
<div id="content" class="content">
<h1 class="title">CSCI 5451 Assignment 2: Distributed and Shared Memory Programming</h1>
<ul class="org-ul">
<li><b>Due: Wed 12-Apr by 11:59 pm</b></li>
<li><i>Approximately 12.5% of total grade</i></li>
<li>Submit to <a href="https://www.gradescope.com/"><b>Gradescope</b></a></li>
<li>You may work in groups of 2 and submit one assignment per group.</li>
</ul>

<div id="outline-container-org77f7670" class="outline-4">
<h4 id="org77f7670">CODE DISTRIBUTION: <a href="a2-code.zip">a2-code.zip</a></h4>
<div class="outline-text-4" id="text-org77f7670">
</div>
</div>

<div id="outline-container-orgfc8df00" class="outline-4">
<h4 id="orgfc8df00">GRADESCOPE TESTS: <a href="a2-gradescope-tests.zip">a2-gradescope-tests.zip</a></h4>
</div>

<div id="outline-container-org5634d5e" class="outline-4">
<h4 id="org5634d5e">CHANGELOG:</h4>
<div class="outline-text-4" id="text-org5634d5e">
<dl class="org-dl">
<dt>Thu Apr 27 05:05:38 PM CDT 2023</dt><dd>The weight on this assignment
incorrectly stated 30%; it is worth 12.5% of the overall grade.</dd>

<dt>Wed Apr 12 08:20:16 AM CDT 2023</dt><dd><p>
The deadline for the project has
been extended to Wed 12-Apr.
</p>

<p>
The Gradescope submission link is open. The automated tests on
Gradescope have no memory checking via Valgrind as this proved
intractable to make work across systems.  The Gradescope tests are
linked here:  <a href="a2-gradescope-tests.zip">a2-gradescope-tests.zip</a>. Running them locally
should reflect he behavior on Gradescope. Add the contents
<code>Makefile-target</code> file's contents to your Makefile and run <code>make
  test-gradescope</code> 
</p></dd>
<dt>Fri Apr  7 02:57:14 PM CDT 2023</dt><dd>The data file
<code>digits_all_3e4.txt</code> was initially missing and has been added into
the codepack. You can also download a stand-alone copy of it here:
<a href="digits_all_3e4.txt">digits_all_3e4.txt</a></dd>
<dt>Wed Apr  5 03:16:50 PM CDT 2023</dt><dd>Minor typo fixes to change to
<code>heat_mpi</code> rather than <code>mpi_heat</code></dd>
<dt>Thu Mar 30 02:04:39 PM CDT 2023</dt><dd>An update for the file
<a href="https://www-users.cse.umn.edu/~kauffman/5451/test_kmeans_serial.org">test<sub>kmeans</sub><sub>serial.org</sub></a> is now available which uses the proper name
for the data directory; it should be <code>mnist-data</code> but was wrong in
the original version.</dd>
</dl>
</div>
</div>

<div id="outline-container-org2de42e8" class="outline-4">
<h4 id="org2de42e8"></h4>
<div class="outline-text-4" id="text-org2de42e8">
<div id="table-of-contents" role="doc-toc">
<h2>Table of Contents</h2>
<div id="text-table-of-contents" role="doc-toc">
<ul>
<li><a href="#org199d205">1. Overview</a></li>
<li><a href="#org7af861f">2. Download Code and Setup</a></li>
<li><a href="#orgaac409c">3. <code>A2-WRITEUP.txt</code> Writeup File</a></li>
<li><a href="#org407fc3b">4. <code>mpi_hello.c</code>: A Sample MPI Program</a></li>
<li><a href="#org881dca3">5. MPI Jobs on MSI Machines</a>
<ul>
<li><a href="#org23928c6">5.1. Software Modules on MSI</a></li>
<li><a href="#orga833656">5.2. Batch Jobs on MSI</a></li>
</ul>
</li>
<li><a href="#org4336be1">6. Testing MPI Codes</a>
<ul>
<li><a href="#orgceb9bf8">6.1. On MSI Machines</a></li>
<li><a href="#org19760ab">6.2. On CSE Labs Machines</a></li>
<li><a href="#org02f6512">6.3. On Home Machines</a></li>
</ul>
</li>
<li><a href="#org1c8ff8d">7. <b>Problem 1</b>: MPI Heat</a>
<ul>
<li><a href="#org61e956f">7.1. The Heat Problem</a></li>
<li><a href="#org81ed113">7.2. MPI Heat</a></li>
<li><a href="#org3c95301">7.3. Features of <code>heat_mpi</code></a></li>
<li><a href="#org9d60649">7.4. Written Summary of the <code>heat_mpi</code> Results</a></li>
<li><a href="#org6785ba8">7.5. Automated Tests for <code>heat_mpi</code></a></li>
<li><a href="#orgf3a1fe7">7.6. Grading Criteria for Problem 1</a></li>
</ul>
</li>
<li><a href="#org063c3c7">8. <b>Problem 2</b>: Serial K-Means Clustering</a></li>
<li><a href="#org616afe0">9. <b>Problem 3</b>: MPI K-Means Clustering</a>
<ul>
<li><a href="#org4e35c3e">9.1. Implementation Requirements</a></li>
<li><a href="#org4f1bc7c">9.2. Grading Criteria for Problem 3</a></li>
</ul>
</li>
<li><a href="#org9bd5ef6">10. Project Submission on Gradescope</a>
<ul>
<li><a href="#org2eee965">10.1. Submit to Gradescope</a></li>
<li><a href="#orgd559585">10.2. Late Policies</a></li>
</ul>
</li>
</ul>
</div>
</div>
</div>
</div>


<div id="outline-container-org199d205" class="outline-2">
<h2 id="org199d205"><span class="section-number-2">1</span> Overview</h2>
<div class="outline-text-2" id="text-1">
<p>
The assignment involves programming in MPI and describing the results
of running your programs with several different parameter sets.  It is
a programming assignment so dust off your C skills.  We have to spent
some class discussing issues related to the assignment but it may be a
good idea to review the lecture videos for when this took place
earlier in the semester. It will pay to start early to get oriented as
debugging parallel programs can be difficult and requires time.
</p>

<p>
There are 3 problems to solve.
</p>
<ol class="org-ol">
<li>Parallelize the heat program from A1 using MPI</li>
<li>Ensure your serial version of K-Means is intact.</li>
<li>Parallelize K-Means clustering using MPI.</li>
</ol>

<p>
For all several of the problems, after finishing your code, you will
need to run some timing experiments and describe the results in a
short text file.
</p>
</div>
</div>

<div id="outline-container-org7af861f" class="outline-2">
<h2 id="org7af861f"><span class="section-number-2">2</span> Download Code and Setup</h2>
<div class="outline-text-2" id="text-2">
<p>
Download the code pack linked at the top of the page. Unzip this which
will create a project folder. Create new files in this
folder. Ultimately you will re-zip this folder to submit it.
</p>

<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">File</th>
<th scope="col" class="org-left">State</th>
<th scope="col" class="org-left">Notes</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">ALL PROBLEMS</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
</tr>

<tr>
<td class="org-left"><code>A2-WRITEUP.txt</code></td>
<td class="org-left">EDIT</td>
<td class="org-left">Fill in timing tables and write answers to discussion questions</td>
</tr>

<tr>
<td class="org-left"><code>Makefile</code></td>
<td class="org-left">Provided</td>
<td class="org-left">Build file to compile all programs</td>
</tr>

<tr>
<td class="org-left"><code>testy</code></td>
<td class="org-left">Testing</td>
<td class="org-left">Test running script</td>
</tr>

<tr>
<td class="org-left"><code>test_mpi.supp</code></td>
<td class="org-left">Testing</td>
<td class="org-left">Suppression file to get Valgrind to hide some library errors</td>
</tr>

<tr>
<td class="org-left"><code>msi-setup.sh</code></td>
<td class="org-left">Provided</td>
<td class="org-left">Source this to set path for MPI on MSI machines</td>
</tr>

<tr>
<td class="org-left"><code>mpi_hello.c</code></td>
<td class="org-left">Provided</td>
<td class="org-left">Demo MPI program along with debug printing function</td>
</tr>
</tbody>
<tbody>
<tr>
<td class="org-left">PROBLEM 1</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
</tr>

<tr>
<td class="org-left"><code>heat_mpi.c</code></td>
<td class="org-left">CREATE</td>
<td class="org-left">Problem 1 parallel version of heat</td>
</tr>

<tr>
<td class="org-left"><code>heat_serial.c</code></td>
<td class="org-left">Provided</td>
<td class="org-left">Problem 1 serial version of the heat problem</td>
</tr>

<tr>
<td class="org-left"><code>heat-slurm.sh</code></td>
<td class="org-left">Provided</td>
<td class="org-left">Problem 2 script to generate timing for heat on MSI</td>
</tr>

<tr>
<td class="org-left"><code>test_heat.org</code></td>
<td class="org-left">Testing</td>
<td class="org-left">Problem 1 tests for heat program</td>
</tr>
</tbody>
<tbody>
<tr>
<td class="org-left">PROBLEM 2</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
</tr>

<tr>
<td class="org-left"><code>kmeans_serial.c</code></td>
<td class="org-left">CREATE</td>
<td class="org-left">Problem 2 serial version of K-means clustering to write in C</td>
</tr>

<tr>
<td class="org-left"><code>kmeans.py</code></td>
<td class="org-left">Provided</td>
<td class="org-left">Problem 2 serial version of K-means clustering in Python</td>
</tr>

<tr>
<td class="org-left"><code>test_kmeans_serial.org</code></td>
<td class="org-left">Testing</td>
<td class="org-left">Problem 2 tests for K-means clustering</td>
</tr>
</tbody>
<tbody>
<tr>
<td class="org-left">PROBLEM 3</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
</tr>

<tr>
<td class="org-left"><code>kmeans_mpi.c</code></td>
<td class="org-left">CREATE</td>
<td class="org-left">Problem 2 serial version of K-means clustering</td>
</tr>

<tr>
<td class="org-left"><code>test_kmeans_mpi.org</code></td>
<td class="org-left">Testing</td>
<td class="org-left">Problem 2 tests for K-means clustering</td>
</tr>

<tr>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
<td class="org-left"><i>NOTE: The <code>test_kmeans_mpi.org</code> file will be released later</i></td>
</tr>
</tbody>
</table>
</div>
</div>


<div id="outline-container-orgaac409c" class="outline-2">
<h2 id="orgaac409c"><span class="section-number-2">3</span> <code>A2-WRITEUP.txt</code> Writeup File</h2>
<div class="outline-text-2" id="text-3">
<p>
Below is a blank copy of the writeup document included in the code
pack. Fill in answers directly into this file as you complete your
programs and submit it as part of your upload.
</p>

<div class="org-src-container">
<pre class="src src-text">                              ____________

                               A2 WRITEUP
                              ____________





GROUP MEMBERS
-------------

  - Member 1: &lt;NAME&gt; &lt;X500&gt;
  - Member 2: &lt;NAME&gt; &lt;X500&gt;

  Up to 2 people may collaborate on this assignment. Write names/x.500
  above. If working alone, leave off Member 2.

  ONLY ONE GROUP MEMBER SHOULD SUBMIT TO GRADESCOPE THEN ADD THEIR
  PARTNER ACCORDING TO INSTRUCTIONS IN THE ASSIGNMENT WEB PAGE.


Problem 1: heat_mpi
===================

heat_mpi Timing Table
~~~~~~~~~~~~~~~~~~~~~

  Fill in the following table on measuring the performance of your
  `heat_mpi' program on MSI's Mesabi machine. Replace 00.00 entries with
  your actual run times. You can use the provided `heat-slurm.sh' script
  to ease this task. Submit it using `sbatch heat-slurm.sh' and extract
  the lines marked `runtime:'.

  -----------------------------
                 Width         
   Procs   6400  25600  102400 
  -----------------------------
       1  00.00  00.00   00.00 
       2  00.00  00.00   00.00 
       4  00.00  00.00   00.00 
       8  00.00  00.00   00.00 
      10  00.00  00.00   00.00 
      16  00.00  00.00   00.00 
      32  00.00  00.00   00.00 
      64  00.00  00.00   00.00 
     128  00.00  00.00   00.00 
  -----------------------------


heat_mpi Discussion Questions
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

  Analyze your table of results and answer the following questions.
  1. Did using more processors result in speedups?
  2. Describe any trends or anomalies you see in the timings and
     speculate on their causes - e.g. was there are a steady increase in
     runtimes, steady decrease, or jagged changes in timing?
  3. Try to explain how number of processors and problem size seem to
     affect runtimes/speedup in the problem. Consider that most Mesabi
     Nodes have 24 cores on them. Comment on whether this number seems
     to affect the performance of the runs you see.


Problem 2: kmeans_serial vs kmeans_mpi
======================================

  Discuss how you chose to parallelize your serial version of K-means in
  the program `kmeans_mpi.c'. Answer the following questions briefly.
  1. How is the input and output data partitioned among the processors?
  2. What communication is required throughout the algorithm?
  3. Which MPI collective communication operations did you employ?


Problem 3: kmeans_mpi
=====================

kmeans_mpi Timing Table
~~~~~~~~~~~~~~~~~~~~~~~

  Fill in the following table on measuring the performance of your
  `kmeans_mpi' program on MSI's Mesabi machine. Replace 00.00 entries
  with your actual run times. You can use the provided `kmeans-slurm.sh'
  script to ease this task.

  The columns are for each of 3 data files that are provided and run in
  the job script.

  digits_all_5e3.txt digits_all_1e4.txt
  -------------------------------------------------------------------
                                       Data File                     
   Procs  digits_all_5e3.txt  digits_all_1e4.txt  digits_all_3e4.txt 
  -------------------------------------------------------------------
       1               00.00               00.00               00.00 
       2               00.00               00.00               00.00 
       4               00.00               00.00               00.00 
       8               00.00               00.00               00.00 
      10               00.00               00.00               00.00 
      16               00.00               00.00               00.00 
      32               00.00               00.00               00.00 
      64               00.00               00.00               00.00 
     128               00.00               00.00               00.00 
  -------------------------------------------------------------------


kmeans_mpi Discussion Questions
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

  Analyze your table of results and answer the following questions.
  1. Did using more processors result in speedups?
  2. Describe any trends or anomalies you see in the timings and
     speculate on their causes - e.g. was there are a steady increase in
     runtimes, steady decrease, or jagged changes in timing?
  3. Try to explain how number of processors and problem size seem to
     affect runtimes/speedup in the problem. Consider that most Mesabi
     Nodes have 24 cores on them. Comment on whether this number seems
     to affect the performance of the runs you see.
</pre>
</div>
</div>
</div>

<div id="outline-container-org407fc3b" class="outline-2">
<h2 id="org407fc3b"><span class="section-number-2">4</span> <code>mpi_hello.c</code>: A Sample MPI Program</h2>
<div class="outline-text-2" id="text-4">
<p>
A simple sample program called <code>mpi_hello.c</code> is provided as part of
the code distribution.  This program includes a useful utility for
debugging purposes.
</p>

<ul class="org-ul">
<li><code>dprintf(fmt,...)</code> is like <code>printf()</code> except that it only prints if
the environment variable <code>DEBUG</code> is set and displays processor
information in the prefixed with <code>|DEBUG</code>. This enables debugging
messages to be printed during development but are disabled during
normal runs.</li>
</ul>

<p>
Compiling and running the program can be done locally on any machine
with MPI installed as follows.
</p>

<div class="org-src-container">
<pre class="src src-sh">&gt;&gt; . mpiopts.sh                       # set the MPIOPTS environment variable

&gt;&gt; mpirun $MPIOTS -np 4 ./mpi_hello   # run the code normally
Hello world from process 0 of 4 (host: val)
Hello from the root processor 0 of 4 (host: val)
Hello world from process 2 of 4 (host: val)
Hello world from process 1 of 4 (host: val)
Hello world from process 3 of 4 (host: val)

&gt;&gt; DEBUG=1 mpirun -np 4 ./mpi_hello  # enable debug messages for this run
Hello world from process 2 of 4 (host: val)
Hello world from process 3 of 4 (host: val)
Hello world from process 1 of 4 (host: val)
|DEBUG Proc 002 / 4 PID 1484871 Host val| Debug message from processor 2
|DEBUG Proc 003 / 4 PID 1484872 Host val| Debug message from processor 3
|DEBUG Proc 001 / 4 PID 1484870 Host val| Debug message from processor 1
Hello world from process 0 of 4 (host: val)
Hello from the root processor 0 of 4 (host: val)
|DEBUG Proc 000 / 4 PID 1484869 Host val| Debug message from processor 0
</pre>
</div>

<p>
Debug printing takes time and should be turned off when reporting
runtimes for programs. Using the shell commands below ensures that the
<code>DEBUG</code> environment variable is unset so debug printing is turned off
for further runs.
</p>
<div class="org-src-container">
<pre class="src src-sh">&gt;&gt; echo $DEBUG                  # check value of DEBUG env var
1                               # currently defined

&gt;&gt; unset DEBUG                  # unset it to remove it
&gt;&gt; echo $DEBUG                  # now has no value

&gt;&gt; mpirun -np 4 ./mpi_hello     # this run has no debug output
P 0: Hello world from process 0 of 4 (host: val)
Hello from the root processor 0 of 4 (host: val)
P 2: Hello world from process 2 of 4 (host: val)
P 3: Hello world from process 3 of 4 (host: val)
P 1: Hello world from process 1 of 4 (host: val)
</pre>
</div>

<p>
NOTE: the <code>dpprintf()</code> function is somewhat inefficient even when
debug output is turned off as it requires calls to <code>getenv()</code>. There
are more efficient alternatives to this that involve macros but that
also involves recompiling code. Since this is a learning exercise we
can tolerate some performance hits in the name of easier debugging. In
the wild you may want to consider alternative debug printing
techniques.
</p>
</div>
</div>

<div id="outline-container-org881dca3" class="outline-2">
<h2 id="org881dca3"><span class="section-number-2">5</span> MPI Jobs on MSI Machines</h2>
<div class="outline-text-2" id="text-5">
<p>
The Minnesota Supercomputing Institute has facilities to run parallel
programs which we will use to evaluate our code. Jobs are handled in
two ways
</p>
<ol class="org-ol">
<li><b>Interactive Jobs</b> which can be run on <i>login</i> nodes. SSH to a
machine, compile your code and run it. You may use a limited number
of processors for this but it is extremely useful while developing
and testing. Login nodes are shared among all users logged in so
performance numbers are unreliable (e.g. affected by other activity
by other users).</li>
<li><b>Batch Jobs</b> which are submitted to a job queue to be run on  via
the <code>sbatch job.sh</code> command. When a job is selected to be run, it
is assigned to run exclusively on some compute nodes so that it may
achieve reliable and maximal performance. This is where timing
results will be reported.</li>
</ol>

<p>
We will use the <code>mesabi</code> MSI system which can be reached via SSH
connections:
</p>
<div class="org-src-container">
<pre class="src src-sh">&gt;&gt; ssh kauffman@mesabi.msi.umn.edu
password: ..........
Duo two-factor login for kauffman

Enter a passcode or select one of the following options:

 1. Duo Push to XXX-XXX-4971
 2. Phone call to XXX-XXX-4971

Passcode or option (1-2): 1
...

ln0006 [~]% 
</pre>
</div>
<p>
It is suggested that you  <a href="https://www-users.cse.umn.edu/~kauffman/tutorials/unix-environment.html#ssh-keys">set up SSH keys to ease login</a> to Mesabi
though you will need to continue Duo-authenticating during login.
</p>
</div>

<div id="outline-container-org23928c6" class="outline-3">
<h3 id="org23928c6"><span class="section-number-3">5.1</span> Software Modules on MSI</h3>
<div class="outline-text-3" id="text-5-1">
<p>
Additionally, MSI uses a <b>software modules system</b> to specify use of
different libraries and code packages. To access the most current
version of MPI, you will need to run the following command in your
login shell.
</p>
<div class="org-src-container">
<pre class="src src-sh"># load mpi software into path
ln0006 [~]% module load ompi/4.0.0/gnu-8.2.0-centos7

# alternative using script in code pack directory
ln0006 [~]% source a2-code/msi-setup.sh

# check version of MPI
ln0006 [~]% which mpicc
/panfs/roc/msisoft/openmpi/el6/4.0.0/gnu-8.2.0.CentOS7/bin/mpicc

ln0006 [~]% which mpirun
/panfs/roc/msisoft/openmpi/el6/4.0.0/gnu-8.2.0.CentOS7/bin/mpirun
</pre>
</div>
<p>
Note that the <code>msi-setup.sh</code> script is provided as an alternative and
can be sourced.
</p>

<p>
The list of all modules available is seen via <code>module avail</code>. Note
that the version of MPI we will use is the most current but not the
default. 
</p>
</div>
</div>

<div id="outline-container-orga833656" class="outline-3">
<h3 id="orga833656"><span class="section-number-3">5.2</span> Batch Jobs on MSI</h3>
<div class="outline-text-3" id="text-5-2">
<p>
Use the <code>sbatch job.sh</code> command to submit a job to the SLURM job
queue.  To ease timing evaluations, a job script with required
parameters has been provided for all of the problems. For instance,
for the Heat problem, use the script <code>heat-slurm.sh</code>.  Checking 
on the status of jobs in the queue is done via <code>squeue -u
username</code>. The provided scripts have the following properties:
</p>
<ul class="org-ul">
<li><p>
Require you to <b>change the directory of your code</b> in it but
otherwise do not need to be modified. Look for the line
</p>
<div class="org-src-container">
<pre class="src src-sh">  # ADJUST: location of executable
  cd ~/mesabi-a2-5451/
</pre>
</div>
<p>
in the script and change the target directory to wherever you have
placed your code. Ensure it is compiled ahead of time.
</p></li>
<li>Will run MPI jobs for the required programs on the required number
of processors</li>
<li>Output for the job will be left in a file like
<code>heat-slurm.sh.job-156425531.out</code> which starts with the job script
name</li>
<li><p>
Lines in the script that are marked <code>runtime:</code> report wall clock
time along with the problem parameters in it these appear like
</p>
<pre class="example">
runtime: procs 16 width 102400 realtime 5.15
</pre>

<p>
and make it possible to use <code>grep</code> or other tools to quickly fish
out the lines reporting runtimes for construction of the results.
</p></li>
<li>Will charge compute time to the CSCI 5451 class account via the
<code>--account</code> option to SLURM</li>
<li>The scripts have the correct MPI software module loaded at the
beginning of them and will call <code>mpirun</code> on the program.</li>
</ul>


<p>
The session below demonstrates use of <code>sbatch</code> and <code>squeue</code>
</p>

<div class="org-src-container">
<pre class="src src-sh">ln0006 [mesabi-a2-5451]% sbatch heat-slurm.sh    # submit script to run for heat jobs
Submitted batch job 156504577

ln0006 [mesabi-a2-5451]% squeue -u kauffman      # check on the submitted jobs by user kauffman
             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)
         156504535     small kmeans-s kauffman PD       0:00      6 (Priority)
         156504577     small heat-slu kauffman PD       0:00      6 (None)

# jobs are PD - Pending but not being run yet


# a useful command to "watch" the queue, press Ctl-c to kill the watch
ln0006 [mesabi-a2-5451]% watch 'squeue -u kauffman'

# later....
ln0006 [mesabi-a2-5451]% squeue -u kauffman      # check on the submitted jobs by user kauffman
             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)

ln0006 [mesabi-a2-5451]%                         # no jobs listed, all jobs done with results

ln0006 [mesabi-a2-5451]% ls *156504577*          # check for output job
heat-slurm.sh.job-156504577.out
</pre>
</div>
</div>
</div>
</div>


<div id="outline-container-org4336be1" class="outline-2">
<h2 id="org4336be1"><span class="section-number-2">6</span> Testing MPI Codes</h2>
<div class="outline-text-2" id="text-6">
</div>
<div id="outline-container-orgceb9bf8" class="outline-3">
<h3 id="orgceb9bf8"><span class="section-number-3">6.1</span> On MSI Machines</h3>
<div class="outline-text-3" id="text-6-1">
<p>
Testing out codes on MSI machines is ideal as it is the same
environment as what the timing evaluation will involve. The same
commands can be used (load MPI module, compile, run with <code>mpirun -np
4</code>), with the minor limitation that there is a cap on the number of
processors that can be tested interactively.
</p>
</div>
</div>

<div id="outline-container-org19760ab" class="outline-3">
<h3 id="org19760ab"><span class="section-number-3">6.2</span> On CSE Labs Machines</h3>
<div class="outline-text-3" id="text-6-2">
<p>
CSE Labs machines have an MPI installation (sort of) set up on the
machines <code>cuda01-cselabs.umn.edu</code> to <code>cuda05-cselabs.umn.edu</code>. By
logging in, one will have access on those machines to <code>mpicc</code> and
<code>mpirun</code> commands. However, the configuration is a bit unwieldly and
requires some setup. Here are some gotchyas to keep in mind and move
you towards the MSI machines.
</p>
<ol class="org-ol">
<li>You must use a Hostfile via <code>mpirun -hostfile hf.txt</code> and due to
the configuration problems on these systems, the hosts will need to
be listed via their IP address rather than their symbolic name.</li>
<li>You will need to have SSH keys setup up for password-free
login. For details on how to do this see <a href="https://www-users.cse.umn.edu/~kauffman/tutorials/unix-environment.html#ssh-keys">this tutorial</a>.</li>
<li>All of the <code>cuda-NN</code> machines will need to be in your authorized
host file. The easiest way to do this is to log in to <code>cuda01</code>
then, from that machine, SSH to <code>cuda02</code>, then <code>cuda03</code>, and so
forth up to <code>cuda05</code>.  This only needs to be done once but if you
fail to do so, MPI runs beyond the local number of processors will
hang silently.</li>
</ol>
<p>
Surmounting these factors is not difficult but creates enough friction
that MSI is easier and preferred for testing.
</p>
</div>
</div>

<div id="outline-container-org02f6512" class="outline-3">
<h3 id="org02f6512"><span class="section-number-3">6.3</span> On Home Machines</h3>
<div class="outline-text-3" id="text-6-3">
<p>
Setting up a home MPI installation is another option. In most cases a
Linux or Unix system can install OpenMPI with a package manager and by
using <code>mpirun --oversubscribe</code> one can test on a number of processors
larger than available as physical cores on the system. Do not expect
speedup in these cases but it is extremely useful for debugging.
</p>
</div>
</div>
</div>

<div id="outline-container-org1c8ff8d" class="outline-2">
<h2 id="org1c8ff8d"><span class="section-number-2">7</span> <b>Problem 1</b>: MPI Heat</h2>
<div class="outline-text-2" id="text-7">
</div>
<div id="outline-container-org61e956f" class="outline-3">
<h3 id="org61e956f"><span class="section-number-3">7.1</span> The Heat Problem</h3>
<div class="outline-text-3" id="text-7-1">
<p>
A slightly modified version of the heat propagation simulation from
HW1 and in-class discussion is in the code pack and called
<code>heat_serial.c</code>. This program can be compiled and run with the
provided <code>Makefile</code> as follows.
</p>

<div class="org-src-container">
<pre class="src src-sh">&gt;&gt; make heat_serial             # build program
gcc -g -Wall -o heat_serial heat_serial.c

&gt;&gt; ./heat_serial                # run with no args to show help info
usage: ./heat_serial max_time width print
  max_time: int
  width: int
  print: 1 print output, 0 no printing

&gt;&gt; ./heat_serial 10 8 1         # run for 10 timesteps with 8 "elements"
   |     0     1     2     3     4     5     6     7 
---+-------------------------------------------------
  0|  20.0  50.0  50.0  50.0  50.0  50.0  50.0  10.0 
  1|  20.0  35.0  50.0  50.0  50.0  50.0  30.0  10.0 
  2|  20.0  35.0  42.5  50.0  50.0  40.0  30.0  10.0 
  3|  20.0  31.2  42.5  46.2  45.0  40.0  25.0  10.0 
  4|  20.0  31.2  38.8  43.8  43.1  35.0  25.0  10.0 
  5|  20.0  29.4  37.5  40.9  39.4  34.1  22.5  10.0 
  6|  20.0  28.8  35.2  38.4  37.5  30.9  22.0  10.0 
  7|  20.0  27.6  33.6  36.3  34.7  29.8  20.5  10.0 
  8|  20.0  26.8  32.0  34.1  33.0  27.6  19.9  10.0 
  9|  20.0  26.0  30.5  32.5  30.9  26.5  18.8  10.0 

&gt;&gt; ./heat_serial 10 8 0         # same run but don't print output, useful for timing as output takes a while

&gt;&gt; ./heat_serial 12 5 1         # run for 12 timesteps with 5 columns / elements
   |     0     1     2     3     4 
---+-------------------------------
  0|  20.0  50.0  50.0  50.0  10.0 
  1|  20.0  35.0  50.0  30.0  10.0 
  2|  20.0  35.0  32.5  30.0  10.0 
  3|  20.0  26.2  32.5  21.2  10.0 
  4|  20.0  26.2  23.8  21.2  10.0 
  5|  20.0  21.9  23.8  16.9  10.0 
  6|  20.0  21.9  19.4  16.9  10.0 
  7|  20.0  19.7  19.4  14.7  10.0 
  8|  20.0  19.7  17.2  14.7  10.0 
  9|  20.0  18.6  17.2  13.6  10.0 
 10|  20.0  18.6  16.1  13.6  10.0 
 11|  20.0  18.0  16.1  13.0  10.0 
</pre>
</div>
</div>
</div>

<div id="outline-container-org81ed113" class="outline-3">
<h3 id="org81ed113"><span class="section-number-3">7.2</span> MPI Heat</h3>
<div class="outline-text-3" id="text-7-2">
<p>
In our <a href="https://canvas.umn.edu/courses/355247/pages/week-03-videos">Thu 02-Feb Lecture we discussed an MPI version of the heat
transfer problem</a> and it may be worthwhile to review this lecture to
recall some details of data distribution and communication.
</p>

<p>
The central task of this problem is to create an MPI version of this
program named <code>heat_mpi</code> which performs the same task but uses MPI
calls to perform the heat calculations on distributed memory
machines. Once completed, this program can be run as follows.
</p>

<div class="org-src-container">
<pre class="src src-sh">&gt;&gt; make heat_mpi                               # build MPI version of heat program
mpicc -g -Wall -o heat_mpi heat_mpi.c

&gt;&gt; source mpiopts.sh                           # set the MPIOPTS env variable, used to suppress warnings

&gt;&gt; mpirun $MPIOPTS -np 2 ./heat_mpi 10 8 1     # run using 2 procs, 10 steps, 8 elements = 4 per proc
   |     0     1     2     3     4     5     6     7 
---+-------------------------------------------------
  0|  20.0  50.0  50.0  50.0  50.0  50.0  50.0  10.0 
  1|  20.0  35.0  50.0  50.0  50.0  50.0  30.0  10.0 
  2|  20.0  35.0  42.5  50.0  50.0  40.0  30.0  10.0 
  3|  20.0  31.2  42.5  46.2  45.0  40.0  25.0  10.0 
  4|  20.0  31.2  38.8  43.8  43.1  35.0  25.0  10.0 
  5|  20.0  29.4  37.5  40.9  39.4  34.1  22.5  10.0 
  6|  20.0  28.8  35.2  38.4  37.5  30.9  22.0  10.0 
  7|  20.0  27.6  33.6  36.3  34.7  29.8  20.5  10.0 
  8|  20.0  26.8  32.0  34.1  33.0  27.6  19.9  10.0 
  9|  20.0  26.0  30.5  32.5  30.9  26.5  18.8  10.0 

&gt;&gt; mpirun $MPIOPTS -np 4 ./heat_mpi 6 12 1     # run using 4 procs, 6 steps, 12 elements = 3 per proc 
   |     0     1     2     3     4     5     6     7     8     9    10    11 
---+-------------------------------------------------------------------------
  0|  20.0  50.0  50.0  50.0  50.0  50.0  50.0  50.0  50.0  50.0  50.0  10.0 
  1|  20.0  35.0  50.0  50.0  50.0  50.0  50.0  50.0  50.0  50.0  30.0  10.0 
  2|  20.0  35.0  42.5  50.0  50.0  50.0  50.0  50.0  50.0  40.0  30.0  10.0 
  3|  20.0  31.2  42.5  46.2  50.0  50.0  50.0  50.0  45.0  40.0  25.0  10.0 
  4|  20.0  31.2  38.8  46.2  48.1  50.0  50.0  47.5  45.0  35.0  25.0  10.0 
  5|  20.0  29.4  38.8  43.4  48.1  49.1  48.8  47.5  41.2  35.0  22.5  10.0 

&gt;&gt; time mpirun $MPIOPTS -np 4 ./heat_mpi 6 12 0 # same as above but suppress output and time the run
real	0m0.168s    # wall clock time to report for the run
user	0m0.102s
sys	0m0.073s
</pre>
</div>
</div>
</div>


<div id="outline-container-org3c95301" class="outline-3">
<h3 id="org3c95301"><span class="section-number-3">7.3</span> Features of <code>heat_mpi</code></h3>
<div class="outline-text-3" id="text-7-3">
<ul class="org-ul">
<li>Name your program <code>heat_mpi.c</code> to be compatible with the provided
<code>Makefile</code>. It has a target to build both <code>heat_serial</code> and
<code>heat_mpi</code> if you name the source file <code>heat_mpi.c</code>.</li>
<li><p>
The serial version of the program provided accepts 3 command line arguments:
</p>
<ol class="org-ol">
<li>Number of time steps (rows in output)</li>
<li>Width of the rod in elements (columns of output)</li>
<li>1 or 0 to indicate whether final output should be printed or suppressed.</li>
</ol>
<p>
The MPI version should allow for the same arguments so that runs
like the following will work.
</p>
<div class="org-src-container">
<pre class="src src-sh">  &gt;&gt; mpirun $MPIOPTS -np 4 ./heat_mpit 10 40 1   # 4 procs, 10 timesteps, width 40, show output 
  ...                                           # output for the run

  &gt;&gt; mpirun $MPIOPTS -np 4 ./heat_mpi 10 40 0   # same but no output
  &gt;&gt; 
</pre>
</div></li>
<li><p>
There is a small script called <code>mpiopts.sh</code> which can set options
for MPI runs to suppress unnecessary warning messages. While
experimenting with your programs in a shell, you can source this
script via the following.
</p>
<div class="org-src-container">
<pre class="src src-sh">  &gt;&gt; source mpiopts.sh            # sets variable MPIOPTS
  
  &gt;&gt; echo $MPIOPTS                # show value of MPIOPTS
  --mca opal_warn_on_missing_libcuda 0
  
  &gt;&gt; . mpiopts.sh                 # same as using "source" to execute script in current shell
</pre>
</div></li>
<li>Divide the problem data so that each processor owns only a portion
of the columns of the heat matrix as discussed in class.</li>
<li>Utilize send and receives or the combined <code>MPI_Sendrecv</code> to allow
processors to communicate with neighbors.</li>
<li>Utilize a collective communication operation at the end of the
computation to gather all results on Processor 0 and have it print
out the entire results matrix if command line args indicate this is
necessary.</li>
<li>Verify that the output of your MPI version is identical to the
output of the serial version which is provided. There are a series
of automated tests that help with this which are described later.</li>
<li>To be compatible with the automated tests, <code>heat_mpi</code> must produce
an exit code of 0; e.g. <code>return 0</code> at the end of <code>main()</code> as is done
in <code>heat_serial.c</code>.</li>
<li><p>
Your MPI version is only required to work correctly in the following
situations:
</p>
<ul class="org-ul">
<li>The width of the rod in elements is evenly divisible by the number
of processors being run.</li>
<li>The width of the rod is at least three times the number of
processors so that each processor would have at least 3 columns
associated with it.</li>
</ul>
<p>
That means the following configurations should work or fail as
indicated.
</p>
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-right" />

<col  class="org-right" />

<col  class="org-left" />

<col  class="org-left" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-right">#Procs</th>
<th scope="col" class="org-right">Width</th>
<th scope="col" class="org-left">Works?</th>
<th scope="col" class="org-left">Notes</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-right">1</td>
<td class="org-right">1</td>
<td class="org-left">no</td>
<td class="org-left">not enough cols</td>
</tr>

<tr>
<td class="org-right">1</td>
<td class="org-right">2</td>
<td class="org-left">no</td>
<td class="org-left">not enough cols</td>
</tr>

<tr>
<td class="org-right">1</td>
<td class="org-right">3</td>
<td class="org-left">yes</td>
<td class="org-left">take special care for 1 proc</td>
</tr>

<tr>
<td class="org-right">4</td>
<td class="org-right">4</td>
<td class="org-left">no</td>
<td class="org-left">only 1 column per proc</td>
</tr>

<tr>
<td class="org-right">4</td>
<td class="org-right">8</td>
<td class="org-left">no</td>
<td class="org-left">only 2 columns per proc</td>
</tr>

<tr>
<td class="org-right">4</td>
<td class="org-right">12</td>
<td class="org-left">yes</td>
<td class="org-left">at least 3 cols per proc</td>
</tr>

<tr>
<td class="org-right">4</td>
<td class="org-right">16</td>
<td class="org-left">yes</td>
<td class="org-left">at least 3 cols per proc</td>
</tr>

<tr>
<td class="org-right">4</td>
<td class="org-right">15</td>
<td class="org-left">no</td>
<td class="org-left">uneven cols</td>
</tr>

<tr>
<td class="org-right">3</td>
<td class="org-right">9</td>
<td class="org-left">yes</td>
<td class="org-left">3 cols per proc, evenly divisible</td>
</tr>

<tr>
<td class="org-right">4</td>
<td class="org-right">40</td>
<td class="org-left">yes</td>
<td class="org-left">evenly divisible, &gt;= 3 cols per proc</td>
</tr>
</tbody>
</table>
<p>
Runs that are marked with "no" in the "Works?" column will not be
tested so are free to do anything (segfault, work correctly, print
an error and exit immediately, etc.).
</p></li>
</ul>
</div>
</div>

<div id="outline-container-org9d60649" class="outline-3">
<h3 id="org9d60649"><span class="section-number-3">7.4</span> Written Summary of the <code>heat_mpi</code> Results</h3>
<div class="outline-text-3" id="text-7-4">
<p>
Included with the project code is the file <code>A2-WRITEUP.txt</code> which has
a timing table to fill in and a few discussion questions which should
be answered.
</p>

<p>
<b>Time your runs on MSI's Mesabi Cluster</b>. You can SSH to it via <code>ssh
myX500@mesabi.msi.umn.edu</code>.  Gathering data for the timing table is
eased via the provided <code>heat-slurm.sh</code> job script which will run jobs
with each of the parameters in the timing table listed. Timing results are
printed with the string <code>runtime:</code> prepended to each line. See the
Earlier examples of how to use <code>sbatch</code>, check the queue via <code>squeue</code>
and find the output files for the completed jobs.
</p>
</div>
</div>

<div id="outline-container-org6785ba8" class="outline-3">
<h3 id="org6785ba8"><span class="section-number-3">7.5</span> Automated Tests for <code>heat_mpi</code></h3>
<div class="outline-text-3" id="text-7-5">
<p>
A battery of automated tests are provided to evaluate whether
<code>heat_mpi</code> is producing correct results on some small examples. These
are present in the file <code>test_heat.org</code> and are run via the <code>testy</code>
script.  This can be done manually or via <code>make test-prob1</code>. Compliant
programs will give results that look like the following.
</p>

<div class="org-src-container">
<pre class="src src-sh">&gt;&gt; unset DEBUG                  # enusre that DEBUG output is disabled

&gt;&gt; make test-prob1              # build prob1 program and run tests
mpicc -g -Wall -o heat_mpi heat_mpi.c
./testy test_heat.org
============================================================
== testy test_heat.org
== Running 10 / 10 tests
1)  Procs=1 Width=20           : ok
2)  Procs=1 Width=20 Valgrind  : ok
3)  Procs=2 Width=20           : ok
4)  Procs=2 Width=20 Valgrind  : ok
5)  Procs=2 Width=20 No output : ok
6)  Procs=2 Width=6            : ok
7)  Procs=2 Width=6 Valgrind   : ok
8)  Procs=4 Width=20           : ok
9)  Procs=4 Width=20 Valgrind  : ok
10) Procs=4 Steps=30 Width=40  : ok
============================================================
RESULTS: 10 / 10 tests passed
</pre>
</div>

<p>
Failed tests will provide a results file with information that can be
studied to gain insight into detected problems with the programs. 
</p>

<p>
Tests are limited to 4 processors max. Some tests run codes under
Valgrind to detect memory problems and help diagnose segmentation
faults.
</p>
</div>
</div>

<div id="outline-container-orgf3a1fe7" class="outline-3 grading 40">
<h3 id="orgf3a1fe7"><span class="section-number-3">7.6</span> <a id="org2fd5b46"></a> Grading Criteria for Problem 1&#xa0;&#xa0;&#xa0;<span class="tag"><span class="grading">grading</span>&#xa0;<span class="40">40</span></span></h3>
<div class="outline-text-3" id="text-7-6">
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-right" />

<col  class="org-left" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-right">&#xa0;</th>
<th scope="col" class="org-left">CRITERIA</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-right">10</td>
<td class="org-left">Code compiles via <code>make prob1</code> and passes automated tests via <code>make test-prob1</code></td>
</tr>

<tr>
<td class="org-right">10</td>
<td class="org-left">Cleanly written code with good documentation according to a Manual Inspection</td>
</tr>

<tr>
<td class="org-right">10</td>
<td class="org-left">Written report includes timings table described above</td>
</tr>

<tr>
<td class="org-right">10</td>
<td class="org-left">Written report includes answers to discussion questions written above.</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>

<div id="outline-container-org063c3c7" class="outline-2">
<h2 id="org063c3c7"><span class="section-number-2">8</span> <b>Problem 2</b>: Serial K-Means Clustering</h2>
<div class="outline-text-2" id="text-8">
<p>
Assignment 1 introduced the K-Means clustering algorithm and provided
a Python. If you did not finish your C port of this program, do so now
as the MPI version should be based upon it.
</p>

<p>
Make sure to name your program <code>kmeans_serial.c</code> to be compatible with
the <code>Makefile</code> provided in the codepack. 
</p>

<p>
Some automated tests are provided for the serial program which check
that the output produced by the program and the clusters it generates
match expectations. The tests also evaluate whether there any memory
problems with the program.
</p>
</div>



<div id="outline-container-orgeb670db" class="outline-4 grading 10">
<h4 id="orgeb670db"><a id="org4badab5"></a> Grading Criteria for Problem 2&#xa0;&#xa0;&#xa0;<span class="tag"><span class="grading">grading</span>&#xa0;<span class="10">10</span></span></h4>
<div class="outline-text-4" id="text-orgeb670db">
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-right" />

<col  class="org-left" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-right">&#xa0;</th>
<th scope="col" class="org-left">CRITERIA</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-right">5</td>
<td class="org-left">Code compiles via <code>make prob2</code> and passes automated tests via <code>make test-prob2</code></td>
</tr>

<tr>
<td class="org-right">5</td>
<td class="org-left">Written report includes a description of how the serial code was parallelized</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>

<div id="outline-container-org616afe0" class="outline-2">
<h2 id="org616afe0"><span class="section-number-2">9</span> <b>Problem 3</b>: MPI K-Means Clustering</h2>
<div class="outline-text-2" id="text-9">
<p>
Implement parallel version of the K-means algorithm for distributed
memory using MPI calls. Name your program <code>kmeans_mpi.c</code> to be
compatible with the provided <code>Makefile</code>. We have discussed the basic
strategy for parallelizing the code during lecture on Thu 16-Feb so
review the discussion present there if you need guidance.
</p>
</div>

<div id="outline-container-org4e35c3e" class="outline-3">
<h3 id="org4e35c3e"><span class="section-number-3">9.1</span> Implementation Requirements</h3>
<div class="outline-text-3" id="text-9-1">
<ol class="org-ol">
<li><p>
<code>kmeans_mpi</code> will accept the same command line arguments as the
Serial version. As all MPI programs, it will be launched via
<code>mpirun</code> such as in
</p>
<pre class="example">
mpirun -np 4 kmeans_mpi mnist-data/digits_all_1e2.txt 10 outdir1
</pre></li>
<li>The behavior and output of the MPI version will match the serial
version in format; e.g. printed messages and produced output
files.</li>
<li><p>
All I/O will be performed only by the root processor.  Only the
root Processor 0 will read the data file specified for the
program. It will send required information to other processors that
are needed by them to contribute to the communication. At the end
of the computation, the root processor will be sent all cluster
assignments and cluster centers to be written to files by the root
processor. 
</p>

<i>
<p>
This requirement is for learning purposes only: in a production
setting, one would look to have each processor perform I/O
operations in parallel if the system supported it. Such parallel
I/O would be facilitated by a binary storage format rather than our
text-based version. However, this assignment is designed to gain
practice using MPI calls and distributing data then accumulating
presents a good opportunity for this.
</p>
</i></li>
<li>The algorithm implemented in the MPI version will follow an input
partitioning approach as was discussed in lecture. Other
possibilities exist but input partitioning is the most likely to be
generally useful and lead to speedups.</li>
</ol>
</div>

<div id="outline-container-org1330680" class="outline-4">
<h4 id="org1330680">Implementation Notes</h4>
<div class="outline-text-4" id="text-org1330680">
<ul class="org-ul">
<li>As before, follow the general flow provided in the Python
implementation. The MPI version should produce identical results and
hopefully faster.</li>
<li>In our <a href="https://canvas.umn.edu/courses/355247/pages/week05-videos">Thu Feb-16 lecture</a> we had a class discussion on how to
implement K-Means clustering for distributed memory machines. It may
be worthwhile to review this discussion as it lays out some
information of data distribution and communication required.</li>
</ul>
</div>
</div>
</div>

<div id="outline-container-org4f1bc7c" class="outline-3 grading 50">
<h3 id="org4f1bc7c"><span class="section-number-3">9.2</span> <a id="orgbef2a93"></a> Grading Criteria for Problem 3&#xa0;&#xa0;&#xa0;<span class="tag"><span class="grading">grading</span>&#xa0;<span class="50">50</span></span></h3>
<div class="outline-text-3" id="text-9-2">
<p>
Test cases will be provided for <code>kmeans_mpi.c</code> but will be released
some time after the initial project is distributed. The <code>Makefile</code>
contains a target for this, just requires the missing
<code>test_kmeans_mpi.org</code> file which will have the tests.
</p>

<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-right" />

<col  class="org-left" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-right">&#xa0;</th>
<th scope="col" class="org-left">CRITERIA</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-right">10</td>
<td class="org-left">Code compiles via <code>make prob3</code>, honors command line parameters on interactive runs.</td>
</tr>

<tr>
<td class="org-right">10</td>
<td class="org-left">Passes automated tests via <code>make test-prob3</code></td>
</tr>

<tr>
<td class="org-right">5</td>
<td class="org-left">Cleanly written code with good documentation and demonstrates appropriate use of</td>
</tr>

<tr>
<td class="org-right">&#xa0;</td>
<td class="org-left">MPI function calls to implement the algorithm such as Broadcast, Scatter, Reduction, Gather, etc.</td>
</tr>

<tr>
<td class="org-right">5</td>
<td class="org-left">Code handles data size that is not evenly divided by the number of processors by using appropriate</td>
</tr>

<tr>
<td class="org-right">&#xa0;</td>
<td class="org-left">"vector" versions of MPI calls like <code>MPI_Scatterv()</code></td>
</tr>

<tr>
<td class="org-right">10</td>
<td class="org-left">Written report includes timings table described above</td>
</tr>

<tr>
<td class="org-right">10</td>
<td class="org-left">Written report includes answers to discussion questions written above.</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>

<div id="outline-container-org9bd5ef6" class="outline-2">
<h2 id="org9bd5ef6"><span class="section-number-2">10</span> Project Submission on Gradescope</h2>
<div class="outline-text-2" id="text-10">
<p>
NOTE 1: Submission to Gradescope is not yet open; follow the
instructions in this section when it opens.
</p>

<p>
NOTE 2: The instructions below pertain to another class and some of
the pictures mention "project" and <code>p1-code</code> which in our case is
"assignment" and <code>a2-code</code>.  The instructions apply nonetheless and
boil down to:
</p>
<ol class="org-ol">
<li>Create a zip of your assignment code via <code>make zip</code></li>
<li>Upload the code to Gradescope</li>
<li>Check that the automated tests that run on Gradescope match you
expectations.</li>
<li>Add you partner to your submission. Only one partner should submit
the code.</li>
</ol>
</div>

<div id="outline-container-org2eee965" class="outline-3">
<h3 id="org2eee965"><span class="section-number-3">10.1</span> <a id="org877760f"></a> Submit to Gradescope</h3>
<div class="outline-text-3" id="text-10-1">
<p>
Some of the pictures below mention 'Assignment' which is now 'Project'
and may mention some files that are not part of the current
project. The process of uploading submission is otherwise the same.
</p>

<ol class="org-ol">
<li><p>
In a terminal, change to your project code directory and type <b>make
zip</b> which will create a zip file of your code. A session should
look like this:
</p>
<div class="org-src-container">
<pre class="src src-sh">   &gt; cd Desktop/5451/a2-code      # location of assignment code

   &gt; ls 
   Makefile    dense_pagerank_mpi.c    heat_serial.c
   ...

   &gt; make zip                     # create a zip file using Makefile target
   rm -f a2-code.zip
   cd .. &amp;&amp; zip "a2-code/p1-code.zip" -r "a2-code"
     adding: a2-code/ (stored 0%)
     adding: a2-code/Makefile (deflated 68%)
     adding: a2-code/dense_pagerank_mpi.c (deflated 69%)
     adding: a2-code/test_dense_pagerank_mpi.org (deflated 71%)
     ...
   Zip created in a2-code.zip

   &gt; ls a2-code.zip
   a2-code.zip
</pre>
</div></li>
<li><p>
Log into <a href="https://www.gradescope.com/">Gradescope</a> and locate and click 'Assignment 2' which will
open up submission
</p>
<div class="org-center">

<div id="orgb813bd1" class="figure">
<p><img src="gradescope01.png" alt="gradescope01.png" style="max-width:100%;" />
</p>
</div>
</div></li>
<li><p>
Click on the 'Drag and Drop' text which will open a file selection
dialog; locate and choose your <code>a2-code.zip</code> file
</p>
<div class="org-center">

<div id="orgc5a8c03" class="figure">
<p><img src="gradescope02.png" alt="gradescope02.png" style="max-width:100%;" />
</p>
</div>
</div></li>
<li><p>
This will show the contents of the Zip file and should include your
C source files along with testing files and directories. 
</p>
<div class="org-center">

<div id="org90df66f" class="figure">
<p><img src="gradescope03.png" alt="gradescope03.png" style="max-width:100%;" />
</p>
</div>
</div></li>
<li><p>
Click 'Upload' which will show progress uploading files.  It may
take a few seconds before this dialog closes to indicate that the
upload is successful. Note: there is a limit of 256 files per
upload; normal submissions are not likely to have problems with
this but you may want to make sure that nothing has gone wrong such
as infinite loops creating many files or incredibly large files. 
</p>

<p>
<b>WARNING</b>: There is a limit of 256 files per zip. Doing <code>make zip</code>
will warn if this limit is exceeded but uploading to Gradescope
will fail without any helpful messages if you upload more the 256
files in a zip. 
</p>

<div class="org-center">

<div id="org9ea7030" class="figure">
<p><img src="gradescope04.png" alt="gradescope04.png" style="max-width:100%;" />
</p>
</div>
</div></li>

<li><p>
Once files have successfully uploaded, the Autograder will begin
running the command line tests and recording results.  These are
the same tests that are run via <code>make test</code>.   
</p>
<div class="org-center">

<div id="orgc1ffce4" class="figure">
<p><img src="gradescope05.png" alt="gradescope05.png" style="max-width:100%;" />
</p>
</div>
</div></li>
<li><p>
When the tests have completed, results will be displayed
summarizing scores along with output for each batch of tests.
</p>
<div class="org-center">

<div id="orgf2028ba" class="figure">
<p><img src="gradescope06.png" alt="gradescope06.png" style="max-width:100%;" />
</p>
</div>
</div></li>
<li><b>Don't forget to add you partner to your submission</b> after
uploading. Only one partner needs to submit the code.</li>
</ol>
</div>
</div>

<div id="outline-container-orgd559585" class="outline-3">
<h3 id="orgd559585"><span class="section-number-3">10.2</span> Late Policies</h3>
<div class="outline-text-3" id="text-10-2">
<p>
You may wish to review the policy on late project submission which
will cost 1 Engagement Point per day late. <b>No projects will be
accepted more than 48 hours after the deadline.</b>
</p>

<p>
<a href="https://www-users.cs.umn.edu/~kauffman/5451/syllabus.html">https://www-users.cs.umn.edu/~kauffman/5451/syllabus.html</a>
</p>
</div>
</div>
</div>
</div>
<div id="postamble" class="status">
<hr/> <i> Author: Chris Kauffman (<a href="mailto:kauffman@umn.edu">kauffman@umn.edu</a>) <br/> Date: 2023-04-27 Thu 17:06 <br/> </i>
</div>
</body>
</html>
